{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_file(filepath: str):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(filepath)\n",
    "    return config\n",
    "\n",
    "def setup_aws_env():\n",
    "    config = load_config_file('./aws-config.cfg')\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "    os.environ['AWS_DEFAULT_REGION'] = config['AWS']['AWS_DEFAULT_REGION']\n",
    "setup_aws_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging Set-up\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.info(\"Logging Set-up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write test data to S3 - Reduced size datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yelp_academic_dataset_checkin': PosixPath('data/raw/yelp_academic_dataset_checkin.json'),\n",
       " 'yelp_academic_dataset_user': PosixPath('data/raw/yelp_academic_dataset_user.json'),\n",
       " 'yelp_academic_dataset_business': PosixPath('data/raw/yelp_academic_dataset_business.json'),\n",
       " 'yelp_academic_dataset_tip': PosixPath('data/raw/yelp_academic_dataset_tip.json'),\n",
       " 'yelp_academic_dataset_review': PosixPath('data/raw/yelp_academic_dataset_review.json')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "bucket_name = 'yelp-customer-reviews'\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "\n",
    "raw_data_path = \"./data/raw\"\n",
    "test_raw_data_path = \"./data/raw-test\"\n",
    "\n",
    "paths = {}\n",
    "for entry in os.listdir(raw_data_path):\n",
    "    if entry.endswith('.json'):\n",
    "        path = Path(raw_data_path) / Path(entry)\n",
    "        paths[path.stem] = path\n",
    "\n",
    "for filename, path in paths.items():\n",
    "    s3_uri = f's3://{bucket_name}/raw-test/{filename}.json'\n",
    "    with open(path, 'r') as f_in:     \n",
    "        with s3.open(s3_uri, 'w') as f_out:\n",
    "            for index, line in enumerate(f_in):\n",
    "                f_out.write(line)\n",
    "                if index > 1000:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data in S3 Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_business.json',\n",
       " 'checkin': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_checkin.json',\n",
       " 'review': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_review.json',\n",
       " 'tip': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_tip.json',\n",
       " 'user': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_user.json'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST = True\n",
    "bucket_name = 'yelp-customer-reviews'\n",
    "root_path = 'raw' if not TEST else 'raw-test'\n",
    "\n",
    "dataset_uris_dict = {}\n",
    "for entry in s3.ls(f\"{bucket_name}/{root_path}\"):\n",
    "    dataset_uris_dict[Path(entry).stem.split('_')[-1]] = f\"s3://{entry}\"\n",
    "dataset_uris_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, month,dayofmonth, year\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "class SparkDF(object):\n",
    "    \"\"\"\n",
    "    Utility class to handle common operation related to Spark Dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath:str):\n",
    "        self.spark = self.create_spark_session()\n",
    "        self.df = self._load_json_data(filepath)\n",
    "        \n",
    "    def create_spark_session(self):\n",
    "        \"\"\"Create a Spark session\"\"\"\n",
    "        os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "\n",
    "        spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "            .getOrCreate()\n",
    "        return spark\n",
    "\n",
    "    def _load_json_data(self, filepath:str):\n",
    "        \"\"\"\n",
    "        Load JSON data from S3 to a Dataframe\n",
    "\n",
    "        Returns:\n",
    "            Spark Dataframe -- Spark dataframe with contents of JSON files\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading file: {filepath}\")\n",
    "            return self.spark.read.json(filepath)\n",
    "        except Exception as e:\n",
    "            if \"No FileSystem for scheme: s3\" in str(e):\n",
    "                logger.warning(\"Switching to slow S3a loading method\")\n",
    "                filepath = filepath.replace(\"s3://\", \"s3a://\")\n",
    "                return self.spark.read.json(filepath)\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def subset_df(self, columns:list, option:str):\n",
    "        if option =='keep':\n",
    "            self.df = self.df.select(*columns)\n",
    "        elif option == 'drop':\n",
    "            self.df = self.df.drop(*columns)\n",
    "\n",
    "    def _write_to_parquet(self, s3_output_path: str, mode: str = 'overwrite', partitions: list = []):\n",
    "        \"\"\"\n",
    "        Writes Spark Dataframe to S3 in the Parquet Format\n",
    "\n",
    "        Arguments:\n",
    "            s3_output_path {str} -- Output path in S3\n",
    "\n",
    "        Keyword Arguments:\n",
    "            mode {str} -- Writing mode (default: {'overwrite'})\n",
    "            partitions {list} -- List of field to partition the data by (default: {[]})\n",
    "\n",
    "        Raises:\n",
    "            e: Raises any error thrown by the write.parqet method from the Spark dataframe\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(s3_output_path)\n",
    "            self.df.write.parquet(\n",
    "                s3_output_path,\n",
    "                mode=mode,\n",
    "                partitionBy=partitions\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"No FileSystem for scheme: s3\" in str(e):\n",
    "                logger.warning(\"Switching to slow S3 output method\")\n",
    "                s3_output_path = s3_output_path.replace(\"s3://\", \"s3a://\")\n",
    "                self.df.write.parquet(\n",
    "                    s3_output_path,\n",
    "                    mode=mode,\n",
    "                    partitionBy=partitions\n",
    "                )\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_uri = f's3://{bucket_name}/processed'\n",
    "data_lake_uri = f's3://{bucket_name}/data-lake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(SparkDF):\n",
    "\n",
    "    def __init__(self, dataset_uris_dict: dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'user'\n",
    "\n",
    "    def get_partitions(self):\n",
    "        return ['pyear', 'pmonth', 'pday']\n",
    "\n",
    "    def process(self):\n",
    "        self.df = (self.df.\n",
    "                   select(\n",
    "                       '*',\n",
    "                       to_timestamp(\n",
    "                           col('yelping_since'), 'yyyy-MM-dd HH:mm:ss').alias('yelping_since_dt')\n",
    "                   )\n",
    "                   )\n",
    "        self.subset_df(['yelping_since'], option='drop')\n",
    "        self.df = self.df.withColumnRenamed(\n",
    "            \"yelping_since_dt\", \"yelping_since\")\n",
    "\n",
    "    def apply_partitioning(self):\n",
    "        self.df = (self.df\n",
    "                   .withColumn(\"pmonth\", month(\"yelping_since\"))\n",
    "                   .withColumn(\"pyear\", year(\"yelping_since\"))\n",
    "                   .withColumn(\"pday\", dayofmonth(\"yelping_since\"))\n",
    "                   .select('*')\n",
    "                   )\n",
    "\n",
    "    def write_to_s3(self, s3_path: str, partitioned: bool = False):\n",
    "        if partitioned:\n",
    "            partitions = self.get_partitions()\n",
    "        else:\n",
    "            partitions = []\n",
    "\n",
    "        s3_path = f\"{s3_path}/{self.name}\"\n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_user.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/user\n",
      "Switching to slow S3 output method\n",
      "s3://yelp-customer-reviews/data-lake/user\n",
      "Switching to slow S3 output method\n"
     ]
    }
   ],
   "source": [
    "user = User(dataset_uris_dict)\n",
    "user.process()\n",
    "user.write_to_s3(processed_uri, partitioned=False)\n",
    "user.apply_partitioning()\n",
    "user.write_to_s3(data_lake_uri, partitioned=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+-----+------+----+\n",
      "|    name|average_stars|      yelping_since|pyear|pmonth|pday|\n",
      "+--------+-------------+-------------------+-----+------+----+\n",
      "|  Rafael|         3.57|2007-07-06 03:27:11| 2007|     7|   6|\n",
      "|Michelle|         3.84|2008-04-28 01:29:25| 2008|     4|  28|\n",
      "|  Martin|         3.44|2008-08-28 23:40:05| 2008|     8|  28|\n",
      "|    John|         3.08|2008-09-20 00:08:14| 2008|     9|  20|\n",
      "|    Anne|         4.37|2008-08-09 00:30:27| 2008|     8|   9|\n",
      "+--------+-------------+-------------------+-----+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user.df.select('name', 'average_stars', 'yelping_since', 'pyear', 'pmonth', 'pday').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Business(SparkDF):\n",
    "    \n",
    "    def __init__(self, dataset_uris_dict:dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'business'\n",
    "    \n",
    "    def get_partitions(self):\n",
    "        return ['pstate','pcity']\n",
    "    \n",
    "    def process(self):\n",
    "        columns_to_keep = [\n",
    "            'business_id',\n",
    "            'name',\n",
    "            'categories',\n",
    "            'state',\n",
    "            'city',\n",
    "            'address',\n",
    "            'postal_code', \n",
    "            'review_count',\n",
    "            'stars'     \n",
    "        ]\n",
    "        self.subset_df(columns_to_keep, option='keep')\n",
    "            \n",
    "    def apply_partitioning(self):       \n",
    "        self.df = (self.df\n",
    "                   .select('*', \n",
    "                           col(\"state\").alias(\"pstate\"),\n",
    "                           col(\"city\").alias(\"pcity\")\n",
    "                          )\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def write_to_s3(self, s3_path:str, partitioned:bool=False):\n",
    "        if partitioned:\n",
    "            partitions=self.get_partitions()\n",
    "        else:\n",
    "            partitions=[]\n",
    "        \n",
    "        s3_path =  f\"{s3_path}/{self.name}\" \n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_business.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/business\n",
      "Switching to slow S3 output method\n",
      "s3://yelp-customer-reviews/data-lake/business\n",
      "Switching to slow S3 output method\n"
     ]
    }
   ],
   "source": [
    "business = Business(dataset_uris_dict)\n",
    "business.process()\n",
    "business.write_to_s3(processed_uri, partitioned=False)\n",
    "business.apply_partitioning()\n",
    "business.write_to_s3(data_lake_uri, partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review(SparkDF):\n",
    "    \n",
    "    def __init__(self, dataset_uris_dict:dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'review'\n",
    "    \n",
    "    def get_partitions(self):\n",
    "        return ['pyear','pmonth', 'pday']\n",
    "    \n",
    "    def process(self):\n",
    "        self.df = (self.df.\n",
    "                   select(\n",
    "                       '*',\n",
    "                       to_timestamp(col('date'), 'yyyy-MM-dd HH:mm:ss').alias('dt')\n",
    "                   )\n",
    "                  )\n",
    "        self.subset_df(['date'], option='drop')\n",
    "    \n",
    "    \n",
    "    def apply_partitioning(self):                     \n",
    "        self.df = (self.df\n",
    "                   .withColumn(\"pmonth\", month(\"dt\"))\n",
    "                           .withColumn(\"pyear\", year(\"dt\"))\n",
    "                           .withColumn(\"pday\", dayofmonth(\"dt\"))\n",
    "                   .select('*')\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def write_to_s3(self, s3_path:str, partitioned:bool=False):\n",
    "        if partitioned:\n",
    "            partitions=self.get_partitions()\n",
    "        else:\n",
    "            partitions=[]\n",
    "        \n",
    "        s3_path =  f\"{s3_path}/{self.name}\" \n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_review.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/review\n",
      "Switching to slow S3 output method\n",
      "s3://yelp-customer-reviews/data-lake/review\n",
      "Switching to slow S3 output method\n"
     ]
    }
   ],
   "source": [
    "review = Review(dataset_uris_dict)\n",
    "review.process()\n",
    "review.write_to_s3(processed_uri, partitioned=False)\n",
    "review.apply_partitioning()\n",
    "review.write_to_s3(data_lake_uri, partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+------+----+\n",
      "|stars|                text|pyear|pmonth|pday|\n",
      "+-----+--------------------+-----+------+----+\n",
      "|  2.0|As someone who ha...| 2015|     4|   4|\n",
      "|  1.0|I am actually hor...| 2013|    12|   7|\n",
      "|  5.0|I love Deagan's. ...| 2015|    12|   7|\n",
      "|  1.0|Dismal, lukewarm,...| 2011|     5|   6|\n",
      "|  4.0|Oh happy day, fin...| 2017|     1|   7|\n",
      "+-----+--------------------+-----+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.df.select('stars','text', 'pyear', 'pmonth', 'pday').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tip(SparkDF):\n",
    "    \n",
    "    def __init__(self, dataset_uris_dict:dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'tip'\n",
    "    \n",
    "    def get_partitions(self):\n",
    "        return ['pyear','pmonth', 'pday']\n",
    "    \n",
    "    def process(self):\n",
    "        self.df = (self.df.\n",
    "                   select(\n",
    "                       '*',\n",
    "                       to_timestamp(col('date'), 'yyyy-MM-dd HH:mm:ss').alias('dt')\n",
    "                   )\n",
    "                  )\n",
    "        self.subset_df(['date'], option='drop')\n",
    "    \n",
    "    \n",
    "    def apply_partitioning(self):                     \n",
    "        self.df = (self.df\n",
    "                   .withColumn(\"pmonth\", month(\"dt\"))\n",
    "                           .withColumn(\"pyear\", year(\"dt\"))\n",
    "                           .withColumn(\"pday\", dayofmonth(\"dt\"))\n",
    "                   .select('*')\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def write_to_s3(self, s3_path:str, partitioned:bool=False):\n",
    "        if partitioned:\n",
    "            partitions=self.get_partitions()\n",
    "        else:\n",
    "            partitions=[]\n",
    "        \n",
    "        s3_path =  f\"{s3_path}/{self.name}\" \n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_tip.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/review\n",
      "Switching to slow S3 output method\n",
      "s3://yelp-customer-reviews/data-lake/review\n",
      "Switching to slow S3 output method\n"
     ]
    }
   ],
   "source": [
    "tip = Tip(dataset_uris_dict)\n",
    "tip.process()\n",
    "tip.write_to_s3(processed_uri, partitioned=False)\n",
    "tip.apply_partitioning()\n",
    "tip.write_to_s3(data_lake_uri, partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- compliment_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- pmonth: integer (nullable = true)\n",
      " |-- pyear: integer (nullable = true)\n",
      " |-- pday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tip.df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+--------------------+-------------------+------+-----+----+\n",
      "|         business_id|compliment_count|                text|             user_id|                 dt|pmonth|pyear|pday|\n",
      "+--------------------+----------------+--------------------+--------------------+-------------------+------+-----+----+\n",
      "|UYX5zL_Xj9WEc_Wp-...|               0|Here for a quick mtg|hf27xTME3EiCp6NL6...|2013-11-26 18:20:08|    11| 2013|  26|\n",
      "|Ch3HkwQYv1YKw_FO0...|               0|Cucumber strawber...|uEvusDwoSymbJJ0au...|2014-06-15 22:26:45|     6| 2014|  15|\n",
      "|rDoT-MgxGRiYqCmi0...|               0|Very nice good se...|AY-laIws3S7YXNl_f...|2016-07-18 22:03:42|     7| 2016|  18|\n",
      "|OHXnDV01gLokiX1EL...|               0|It's a small plac...|Ue_7yUlkEbX4AhnYd...|2014-06-06 01:10:34|     6| 2014|   6|\n",
      "|GMrwDXRlAZU2zj5nH...|               0|8 sandwiches, $24...|LltbT_fUMqZ-ZJP-v...|2011-04-08 18:12:01|     4| 2011|   8|\n",
      "+--------------------+----------------+--------------------+--------------------+-------------------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tip.df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
