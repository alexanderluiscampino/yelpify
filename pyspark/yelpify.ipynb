{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
      "Collecting botocore>=1.12.91\n",
      "  Downloading botocore-1.17.20-py2.py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.7.4)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore>=1.12.91->s3fs) (1.25.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.15.0)\n",
      "Installing collected packages: docutils, jmespath, botocore, s3fs\n",
      "Successfully installed botocore-1.17.20 docutils-0.15.2 jmespath-0.10.0 s3fs-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_file(filepath: str):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(filepath)\n",
    "    return config\n",
    "\n",
    "def setup_aws_env():\n",
    "    config = load_config_file('./aws-config.cfg')\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "    os.environ['AWS_DEFAULT_REGION'] = config['AWS']['AWS_DEFAULT_REGION']\n",
    "setup_aws_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging Set-up\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.info(\"Logging Set-up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "bucket_name = 'yelp-customer-reviews'\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write test data to S3 - Reduced size datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yelp_academic_dataset_checkin': PosixPath('data/raw/yelp_academic_dataset_checkin.json'),\n",
       " 'yelp_academic_dataset_user': PosixPath('data/raw/yelp_academic_dataset_user.json'),\n",
       " 'yelp_academic_dataset_business': PosixPath('data/raw/yelp_academic_dataset_business.json'),\n",
       " 'yelp_academic_dataset_tip': PosixPath('data/raw/yelp_academic_dataset_tip.json'),\n",
       " 'yelp_academic_dataset_review': PosixPath('data/raw/yelp_academic_dataset_review.json')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "raw_data_path = \"./data/raw\"\n",
    "test_raw_data_path = \"./data/raw-test\"\n",
    "\n",
    "paths = {}\n",
    "for entry in os.listdir(raw_data_path):\n",
    "    if entry.endswith('.json'):\n",
    "        path = Path(raw_data_path) / Path(entry)\n",
    "        paths[path.stem] = path\n",
    "\n",
    "for filename, path in paths.items():\n",
    "    s3_uri = f's3://{bucket_name}/raw-test/{filename}.json'\n",
    "    with open(path, 'r') as f_in:     \n",
    "        with s3.open(s3_uri, 'w') as f_out:\n",
    "            for index, line in enumerate(f_in):\n",
    "                f_out.write(line)\n",
    "                if index > 1000:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data in S3 Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_business.json',\n",
       " 'checkin': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_checkin.json',\n",
       " 'review': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_review.json',\n",
       " 'tip': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_tip.json',\n",
       " 'user': 's3://yelp-customer-reviews/raw-test/yelp_academic_dataset_user.json'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST = True\n",
    "bucket_name = 'yelp-customer-reviews'\n",
    "root_path = 'raw' if not TEST else 'raw-test'\n",
    "\n",
    "dataset_uris_dict = {}\n",
    "for entry in s3.ls(f\"{bucket_name}/{root_path}\"):\n",
    "    dataset_uris_dict[Path(entry).stem.split('_')[-1]] = f\"s3://{entry}\"\n",
    "dataset_uris_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_uri = f's3://{bucket_name}/processed'\n",
    "data_lake_uri = f's3://{bucket_name}/data-lake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_directory_to_s3(local_directory:str, bucket_name:str, root_prefix:str, filetype:str):\n",
    "    path = Path(local_directory)\n",
    "    \n",
    "    for index, entry in enumerate(path.rglob(f'*.{filetype}')):\n",
    "        local_path = str(entry)\n",
    "        object_key = local_path.replace(str(path), root_prefix)\n",
    "        s3_uri = f's3://{bucket_name}/{object_key}'\n",
    "        print(s3_uri)\n",
    "        s3.put(str(entry), s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, month,dayofmonth, year\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "class SparkDF(object):\n",
    "    \"\"\"\n",
    "    Utility class to handle common operation related to Spark Dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath:str):\n",
    "        self.spark = self.create_spark_session()\n",
    "        self.df = self._load_json_data(filepath)\n",
    "        \n",
    "    def create_spark_session(self):\n",
    "        \"\"\"Create a Spark session\"\"\"\n",
    "        os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "\n",
    "        spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "            .getOrCreate()\n",
    "        return spark\n",
    "\n",
    "    def _load_json_data(self, filepath:str):\n",
    "        \"\"\"\n",
    "        Load JSON data from S3 to a Dataframe\n",
    "\n",
    "        Returns:\n",
    "            Spark Dataframe -- Spark dataframe with contents of JSON files\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading file: {filepath}\")\n",
    "            return self.spark.read.json(filepath)\n",
    "        except Exception as e:\n",
    "            if \"No FileSystem for scheme: s3\" in str(e):\n",
    "                logger.warning(\"Switching to slow S3a loading method\")\n",
    "                filepath = filepath.replace(\"s3://\", \"s3a://\")\n",
    "                return self.spark.read.json(filepath)\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def subset_df(self, columns:list, option:str):\n",
    "        if option =='keep':\n",
    "            self.df = self.df.select(*columns)\n",
    "        elif option == 'drop':\n",
    "            self.df = self.df.drop(*columns)\n",
    "\n",
    "    def _write_to_parquet(self, s3_output_path: str, mode: str = 'overwrite', partitions: list = []):\n",
    "        \"\"\"\n",
    "        Writes Spark Dataframe to S3 in the Parquet Format\n",
    "\n",
    "        Arguments:\n",
    "            s3_output_path {str} -- Output path in S3\n",
    "\n",
    "        Keyword Arguments:\n",
    "            mode {str} -- Writing mode (default: {'overwrite'})\n",
    "            partitions {list} -- List of field to partition the data by (default: {[]})\n",
    "\n",
    "        Raises:\n",
    "            e: Raises any error thrown by the write.parqet method from the Spark dataframe\n",
    "        \"\"\"\n",
    "        local_temp_dir = Path('./temp')\n",
    "        os.makedirs(local_temp_dir, exist_ok=True)\n",
    "        bucket_name, root_prefix, _ = s3.split_path(s3_output_path)\n",
    "           \n",
    "        try:\n",
    "            logger.info(s3_output_path)\n",
    "            self.df.write.parquet(\n",
    "                str(local_temp_dir),\n",
    "                mode=mode,\n",
    "                partitionBy=partitions\n",
    "            )\n",
    "            move_directory_to_s3(local_temp_dir, bucket_name, root_prefix, 'parquet')\n",
    "            rmtree(local_temp_dir)\n",
    "        except Exception as e:\n",
    "            if \"No FileSystem for scheme: s3\" in str(e):\n",
    "                logger.warning(\"Switching to slow S3 output method\")\n",
    "                s3_output_path = s3_output_path.replace(\"s3://\", \"s3a://\")\n",
    "                self.df.write.parquet(\n",
    "                    s3_output_path,\n",
    "                    mode=mode,\n",
    "                    partitionBy=partitions\n",
    "                )\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(SparkDF):\n",
    "\n",
    "    def __init__(self, dataset_uris_dict: dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'user'\n",
    "\n",
    "    def get_partitions(self):\n",
    "        return ['pyear', 'pmonth', 'pday']\n",
    "\n",
    "    def process(self):\n",
    "        self.subset_df([\n",
    "                        'friends',\n",
    "                       'compliment_cool',\n",
    "                       'compliment_cute',\n",
    "                       'compliment_funny',\n",
    "                       'compliment_hot',\n",
    "                       'compliment_list',\n",
    "                       'compliment_more',\n",
    "                       'compliment_note',\n",
    "                       'compliment_photos',\n",
    "                       'compliment_plain',\n",
    "                       'compliment_profile',\n",
    "                       'compliment_writer',\n",
    "                       'cool',\n",
    "                       'elite',\n",
    "                       'fans',\n",
    "                       'funny',\n",
    "                       'useful'], option='drop')\n",
    "\n",
    "    def apply_partitioning(self):\n",
    "        self.df = (self.df.\n",
    "                   select(\n",
    "                       '*',\n",
    "                       to_timestamp(\n",
    "                           col('yelping_since'), 'yyyy-MM-dd HH:mm:ss').alias('yelping_since_dt')\n",
    "                   )\n",
    "                   )\n",
    "        self.df = (self.df\n",
    "                   .withColumn(\"pmonth\", month(\"yelping_since_dt\"))\n",
    "                   .withColumn(\"pyear\", year(\"yelping_since_dt\"))\n",
    "                   .withColumn(\"pday\", dayofmonth(\"yelping_since_dt\"))\n",
    "                   .select('*')\n",
    "                   )\n",
    "        self.subset_df(['yelping_since_dt'], option='drop')\n",
    "\n",
    "    def write_to_s3(self, s3_path: str, partitioned: bool = False):\n",
    "        if partitioned:\n",
    "            partitions = self.get_partitions()\n",
    "        else:\n",
    "            partitions = []\n",
    "\n",
    "        s3_path = f\"{s3_path}/{self.name}\"\n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_user.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/user\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://yelp-customer-reviews/processed/user/part-00002-042985d7-dd2c-4435-943b-644d98d4611f-c000.snappy.parquet\n",
      "s3://yelp-customer-reviews/processed/user/part-00001-042985d7-dd2c-4435-943b-644d98d4611f-c000.snappy.parquet\n",
      "s3://yelp-customer-reviews/processed/user/part-00000-042985d7-dd2c-4435-943b-644d98d4611f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "user = User(dataset_uris_dict)\n",
    "user.process()\n",
    "user.write_to_s3(processed_uri, partitioned=False)\n",
    "user.apply_partitioning()\n",
    "# user.write_to_s3(data_lake_uri, partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      " |-- pmonth: integer (nullable = true)\n",
      " |-- pyear: integer (nullable = true)\n",
      " |-- pday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user.df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+-----+------+----+\n",
      "|    name|average_stars|      yelping_since|pyear|pmonth|pday|\n",
      "+--------+-------------+-------------------+-----+------+----+\n",
      "|  Rafael|         3.57|2007-07-06 03:27:11| 2007|     7|   6|\n",
      "|Michelle|         3.84|2008-04-28 01:29:25| 2008|     4|  28|\n",
      "|  Martin|         3.44|2008-08-28 23:40:05| 2008|     8|  28|\n",
      "|    John|         3.08|2008-09-20 00:08:14| 2008|     9|  20|\n",
      "|    Anne|         4.37|2008-08-09 00:30:27| 2008|     8|   9|\n",
      "+--------+-------------+-------------------+-----+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user.df.select('name', 'average_stars', 'yelping_since', 'pyear', 'pmonth', 'pday').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Business(SparkDF):\n",
    "    \n",
    "    def __init__(self, dataset_uris_dict:dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'business'\n",
    "    \n",
    "    def get_partitions(self):\n",
    "        return ['pstate','pcity']\n",
    "    \n",
    "    def process(self):\n",
    "        columns_to_keep = [\n",
    "            'business_id',\n",
    "            'name',\n",
    "            'categories',\n",
    "            'state',\n",
    "            'city',\n",
    "            'address',\n",
    "            'postal_code', \n",
    "            'review_count',\n",
    "            'stars'     \n",
    "        ]\n",
    "        self.subset_df(columns_to_keep, option='keep')\n",
    "            \n",
    "    def apply_partitioning(self):       \n",
    "        self.df = (self.df\n",
    "                   .select('*', \n",
    "                           col(\"state\").alias(\"pstate\"),\n",
    "                           col(\"city\").alias(\"pcity\")\n",
    "                          )\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def write_to_s3(self, s3_path:str, partitioned:bool=False):\n",
    "        if partitioned:\n",
    "            partitions=self.get_partitions()\n",
    "        else:\n",
    "            partitions=[]\n",
    "        \n",
    "        s3_path =  f\"{s3_path}/{self.name}\" \n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- pstate: string (nullable = true)\n",
      " |-- pcity: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+-----+---------------+--------------------+-----------+------------+-----+------+---------------+\n",
      "|         business_id|                name|          categories|state|           city|             address|postal_code|review_count|stars|pstate|          pcity|\n",
      "+--------------------+--------------------+--------------------+-----+---------------+--------------------+-----------+------------+-----+------+---------------+\n",
      "|f9NumwFMBDn751xgF...|The Range At Lake...|Active Life, Gun/...|   NC|      Cornelius|     10913 Bailey Rd|      28031|          36|  3.5|    NC|      Cornelius|\n",
      "|Yzvjg0SayhoZgCljU...|   Carlos Santo, NMD|Health & Medical,...|   AZ|     Scottsdale|8880 E Via Linda,...|      85258|           4|  5.0|    AZ|     Scottsdale|\n",
      "|XNoUzKckATkOD1hP6...|             Felinus|Pets, Pet Service...|   QC|       Montreal|3554 Rue Notre-Da...|    H4C 1P4|           5|  5.0|    QC|       Montreal|\n",
      "|6OAZjbxqM5ol29BuH...|Nevada House of Hose|Hardware Stores, ...|   NV|North Las Vegas|      1015 Sharp Cir|      89030|           3|  2.5|    NV|North Las Vegas|\n",
      "|51M2Kk903DFYI6gnB...|USE MY GUY SERVIC...|Home Services, Pl...|   AZ|           Mesa|  4827 E Downing Cir|      85205|          26|  4.5|    AZ|           Mesa|\n",
      "+--------------------+--------------------+--------------------+-----+---------------+--------------------+-----------+------------+-----+------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "business.df.printSchema()\n",
    "business.df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_business.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/business\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://yelp-customer-reviews/processed/business/part-00000-47cd7fa7-bfe3-473b-ae8f-0d01ad276f47-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "business = Business(dataset_uris_dict)\n",
    "business.process()\n",
    "business.write_to_s3(processed_uri, partitioned=False)\n",
    "business.apply_partitioning()\n",
    "# business.write_to_s3(data_lake_uri, partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review(SparkDF):\n",
    "    \n",
    "    def __init__(self, dataset_uris_dict:dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'review'\n",
    "    \n",
    "    def get_partitions(self):\n",
    "        return ['pyear','pmonth', 'pday']\n",
    "    \n",
    "    def process(self):\n",
    "        self.subset_df(['cool',\n",
    "                       'funny',\n",
    "                       'useful'\n",
    "                       ], option='drop')\n",
    "    \n",
    "    \n",
    "    def apply_partitioning(self):\n",
    "        self.df = (self.df.\n",
    "                   select(\n",
    "                       '*',\n",
    "                       to_timestamp(col('date'), 'yyyy-MM-dd HH:mm:ss').alias('dt')\n",
    "                   )\n",
    "                  )\n",
    "        \n",
    "        self.df = (self.df\n",
    "                   .withColumn(\"pmonth\", month(\"dt\"))\n",
    "                           .withColumn(\"pyear\", year(\"dt\"))\n",
    "                           .withColumn(\"pday\", dayofmonth(\"dt\"))\n",
    "                   .select('*')\n",
    "            )\n",
    "        self.subset_df(['dt'], option='drop')\n",
    "        \n",
    "        \n",
    "    def write_to_s3(self, s3_path:str, partitioned:bool=False):\n",
    "        if partitioned:\n",
    "            partitions=self.get_partitions()\n",
    "        else:\n",
    "            partitions=[]\n",
    "        \n",
    "        s3_path =  f\"{s3_path}/{self.name}\" \n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_review.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://yelp-customer-reviews/processed/review/part-00000-40b5c334-a129-46aa-a3df-068f4624dc50-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "review = Review(dataset_uris_dict)\n",
    "review.process()\n",
    "review.df = review.df.withColumn('stars', col(\"stars\").cast(StringType()))\n",
    "review.write_to_s3(processed_uri, partitioned=False)\n",
    "review.apply_partitioning()\n",
    "# review.write_to_s3(data_lake_uri, partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('business_id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('categories', 'string'),\n",
       " ('state', 'string'),\n",
       " ('city', 'string'),\n",
       " ('address', 'string'),\n",
       " ('postal_code', 'string'),\n",
       " ('review_count', 'bigint'),\n",
       " ('stars', 'double'),\n",
       " ('pstate', 'string'),\n",
       " ('pcity', 'string')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- pmonth: integer (nullable = true)\n",
      " |-- pyear: integer (nullable = true)\n",
      " |-- pday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "review.df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- pmonth: integer (nullable = true)\n",
      " |-- pyear: integer (nullable = true)\n",
      " |-- pday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+------+----+\n",
      "|stars|                text|pyear|pmonth|pday|\n",
      "+-----+--------------------+-----+------+----+\n",
      "|  2.0|As someone who ha...| 2015|     4|  15|\n",
      "|  1.0|I am actually hor...| 2013|    12|   7|\n",
      "|  5.0|I love Deagan's. ...| 2015|    12|   5|\n",
      "|  1.0|Dismal, lukewarm,...| 2011|     5|  27|\n",
      "|  4.0|Oh happy day, fin...| 2017|     1|  14|\n",
      "+-----+--------------------+-----+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.df.select('stars','text', 'pyear', 'pmonth', 'pday').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tip(SparkDF):\n",
    "    \n",
    "    def __init__(self, dataset_uris_dict:dict):\n",
    "        super().__init__(dataset_uris_dict[self.name])\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'tip'\n",
    "    \n",
    "    def get_partitions(self):\n",
    "        return ['pyear','pmonth', 'pday']\n",
    "    \n",
    "    def process(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def apply_partitioning(self):\n",
    "        self.df = (self.df.\n",
    "                   select(\n",
    "                       '*',\n",
    "                       to_timestamp(col('date'), 'yyyy-MM-dd HH:mm:ss').alias('dt')\n",
    "                   )\n",
    "                  )\n",
    "        \n",
    "        self.df = (self.df\n",
    "                   .withColumn(\"pmonth\", month(\"dt\"))\n",
    "                           .withColumn(\"pyear\", year(\"dt\"))\n",
    "                           .withColumn(\"pday\", dayofmonth(\"dt\"))\n",
    "                   .select('*')\n",
    "            )\n",
    "        self.subset_df(['dt'], option='drop')\n",
    "        \n",
    "        \n",
    "    def write_to_s3(self, s3_path:str, partitioned:bool=False):\n",
    "        if partitioned:\n",
    "            partitions=self.get_partitions()\n",
    "        else:\n",
    "            partitions=[]\n",
    "        \n",
    "        s3_path =  f\"{s3_path}/{self.name}\" \n",
    "        self._write_to_parquet(s3_path, partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file: s3://yelp-customer-reviews/raw-test/yelp_academic_dataset_tip.json\n",
      "Switching to slow S3a loading method\n",
      "s3://yelp-customer-reviews/processed/tip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://yelp-customer-reviews/processed/tip/part-00000-c6e94082-f55b-46e5-809f-176c178614cd-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "tip = Tip(dataset_uris_dict)\n",
    "tip.process()\n",
    "tip.write_to_s3(processed_uri, partitioned=False)\n",
    "# tip.apply_partitioning()\n",
    "# # tip.write_to_s3(data_lake_uri, partitioned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- compliment_count: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tip.df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------------+--------------------+--------------------+\n",
      "|         business_id|compliment_count|               date|                text|             user_id|\n",
      "+--------------------+----------------+-------------------+--------------------+--------------------+\n",
      "|UYX5zL_Xj9WEc_Wp-...|               0|2013-11-26 18:20:08|Here for a quick mtg|hf27xTME3EiCp6NL6...|\n",
      "|Ch3HkwQYv1YKw_FO0...|               0|2014-06-15 22:26:45|Cucumber strawber...|uEvusDwoSymbJJ0au...|\n",
      "|rDoT-MgxGRiYqCmi0...|               0|2016-07-18 22:03:42|Very nice good se...|AY-laIws3S7YXNl_f...|\n",
      "|OHXnDV01gLokiX1EL...|               0|2014-06-06 01:10:34|It's a small plac...|Ue_7yUlkEbX4AhnYd...|\n",
      "|GMrwDXRlAZU2zj5nH...|               0|2011-04-08 18:12:01|8 sandwiches, $24...|LltbT_fUMqZ-ZJP-v...|\n",
      "+--------------------+----------------+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tip.df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
